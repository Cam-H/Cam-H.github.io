<!DOCTYPE HTML>
<!--
	Halcyonic by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>IoT Voice - Cameron Hatherell</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="subpage">
		<div id="page-wrapper">

			<!-- Header -->
				<section id="header">
					<div class="container">
						<div class="row">
							<div class="col-12">

								<!-- Logo -->
									<h1><a href="index.html" id="logo">Cameron Hatherell</a></h1>

								<!-- Nav -->
									<nav id="nav">
										<a href="index.html">Homepage</a>
										<a href="autocarver.html">AutoCarver</a>
										<a href="wasp.html">WASP</a>
										<a href="iotvoice.html">IoT Voice Recognition</a>
										<a href="archive.html">Archive</a>
									</nav>
							</div>
						</div>
					</div>
				</section>

			<!-- Content -->
				<section id="content">
					<div class="container">
						<div class="row">
							<div class="col-9 col-12-medium">

								<!-- Main Content -->
									<section>
									
										<div id="introduction">
											<header>
												<h2>IoT Voice Recognition</h2>
												<h3>A less invasive, offline alternative</h3>
											</header>
											<p>
												Existing technologies like <a href="https://en.wikipedia.org/wiki/Amazon_Alexa">Amazon Alexa</a> act as effective virtual home assistants.
												However, given the requirement to always be connected to the cloud, such systems necessarily introduce a vector for hackers to invade a 
												user's privacy; that is assuming the company is not already using data collected for their own purposes such as marketing or AI training.
												This project seeks to address that issue, developing an entirely offline voice recognition system that may run on an embedded system. In this
												project, any processing is done onboard, and the idea would be to integrate into lighting and other home systems for control in an actual implementation.
											</p>
											
										</div>		
										<div id="highlights">
											<h3>Technical Highlights</h3>
											<ul class="square-list">
												<li>Digital signal processing applied to audio iputs</li>
												<li>Image processing with an artificial neural network</li>
												<li>Hardware integration with ARM Cortex A-9, DE10-Standard development board</li>
											</ul>

										</div>
										<div id="background">
											<h3>Background</h3>
											<p>
												Processing audio signals is a challenging task with an entire dedicated field of study. In the field, the <a href="#ft" class="glossary-link">Fourier Transform</a> is a 
												fundamental function to extract frequency and magnitude information from a signal. Applying the function repeatedly over time on
												an input signal, a <a href="#spectro" class="glossary-link">spectrogram</a> may be developed. A spectrogram is a visual representation of the frequencies and their magnitudes
												over time.
											</p>
											<figure >
												<a href="https://www.researchgate.net/publication/319081627_An_Algorithm_for_Detection_of_Breath_Sounds_in_Spontaneous_Speech_with_Application_to_Speaker_Recognition">
													<img src="images/spectrogram.png" class="center">
												</a>
												<figcaption style="text-align: center">An example spectrogram (frequency magnitude denoted by colour)</figcaption>
											</figure>
											<p>
												Human speech, when converted into a spectrogram, is rather unique. Different words are composed of different frequencies at different magnitudes
												which are observable visually as distinct features. A <a href="#cnn" class="glossary-link">convolutional neural network</a> may be applied to spectrograms, then, to recognize these features
												and thereby infer the words that were originally spoken and recorded.
											</p>
											<p>
												Given a trained neural network, hardware integration is fairly straightforward. With just a microphone for audio recording and an embedded
												system for processing, a primitive voice recognition system is realized. 
											</p>
										</div>
										<div id="training">
											<h3>Training</h3>
											<p>
												<a href="#matlab" class="glossary-link">MATLAB</a> was used to train the convolutional neural network. Google offers a <a href="https://research.google/blog/launching-the-speech-commands-dataset/">dataset</a> for
												speech training which was used for this project. Given a set of 30 words, the neural network was trained over thousands of iterations.
												After training, the network was able to correctly identify approximately 80% of the words.
											</p>
											<figure >
												<img src="images/training.png" class="center">
												<figcaption style="text-align: center">Network training results</figcaption>
											</figure>
										</div>
										<div id="testing">
											<h3>Testing</h3>
											<p>
												MATLAB Coder offers functionality to auto-generate C code based on scripts developed in MATLAB. This feature was leveraged to export the trained neural
												network such that it could be built and deployed to an embedded system. With the addition of a basic microphone, and a bit of extra C code for integration,
												the system was made to run on appropriate hardware.
											</p>
											<p>
												For the purposes of testing, the integrated 7-segment display was used to report what words were detected by the device.
											</p>
											<figure >
												<img src="images/off.jpg" class="center">
												<figcaption style="text-align: center">Key word: 'off' response</figcaption>
											</figure>
											<figure >
												<img src="images/on.jpg" class="center">
												<figcaption style="text-align: center">Key word: 'on' response</figcaption>
											</figure>
											<figure >
												<img src="images/marvin.jpg" class="center">
												<figcaption style="text-align: center">Key word: 'Marvin' response</figcaption>
											</figure>
											<p>
												In testing, it became clear that the embedded device was insufficiently powerful to process audio signals in real time. Between upsampling to match trained data, to 
												applying the discrete Fourier Transform to develop a spectrogram, to evaluating with the neural network, individual words took seconds to process at a time. A button
												was required to limit the window for testing, and the system did perform well despite the slowness.
											</p>
										</div>
										<div id="conclusion">
											<h3>Conclusions</h3>
											<p>
												In its current iteration, the major limiting factor for this project is an unacceptably slow processing speed. Accuracy could also be improved, as only approximately
												80% of interpretations are actually correct; considering this was tested under optimal conditions with clear speaking and minimal noise, the real-world performance is likely even worse.
												These two problems are intrinsically linked as a bigger network would require more processing power but would result in more accurate interpretations. A larger network
												may also be trained with more words and larger datasets that might reduce cases of false positives. For further development, possible avenues might be to:

												<ul class="square-list">
													<li>Continue testing different parameters when constructing the network to find a more efficient configuration</li>
													<li>Research audio feature extraction to select more appropriate signal features for training</li>
													<li>Quantize the neural network to decrease memory usage and processor cycles dedicated to floating-point math</li>
													<li>Match sampling rates between training data and the microphone signal by default</li>
													<li>Utilize a more appropriate microcontroller that better meets speed and power requirements</li>
													<li>Integrate an <a href="https://mythic.ai/products/m1076-analog-matrix-processor/">analog matrix processor</a> to handle the signal classification, significantly reducing the load on the microcontroller</li>
													<li>Perform noise testing using the final product to estimate the effect of different room acoustics and environmental factors that could obfuscate the input signal. Integrate noise reduction filters based on the result</li>
												</ul>
											</p>
										</div>
										<div id="dependencies">
											<h3>Dependencies</h3>
											<p>
												A suite of MATLAB toolboxes were used in the development of the network:
												<ul>
													<li><a href="https://www.mathworks.com/products/signal.html">Signal Processing Toolbox</a> &
														<a href="https://www.mathworks.com/products/dsp-system.html">DSP System Toolbox</a>  - For standard signal processing functions</li>
													<li><a href="https://www.mathworks.com/products/audio.html">Audio Toolbox</a> - For dedicated audio feature extraction functions</li>
													<li><a href="https://www.mathworks.com/help/deeplearning">Deep Learning Toolbox</a> - To develop and train the convolutional neural network</li>
													<li><a href="https://www.mathworks.com/products/matlab-coder.html">MATLAB Coder</a> - To export the network to a C program</li>
												</ul>
											</p>
											<p>
												As mentioned, an existing <a href="https://research.google/blog/launching-the-speech-commands-dataset/">dataset</a> was used as input to train the network.
											</p>
											<p>
												In terms of hardware, the following electronics were used for testing:
												<ul class="square-list">
													<li><a href="https://en.wikipedia.org/wiki/ARM_Cortex-A9">ARM Cortex-A9</a> - Multi-core processor used for testing purposes</li>
													<li><a href="https://www.terasic.com.tw/cgi-bin/page/archive.pl?Language=English&CategoryNo=165&No=1081">DE10-Standard Development board</a> - FPGA board used for testing</li>
													<li>Earbuds with a built in microphone to record audio</li>
												</ul>
											</p>
										</div>
										<div id="glossary">
											<h3>Glossary</h3>
											<ul>
												<li id="cnn"><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Network</a> - A class of feedforward (Input -> Output) neural network and the standard for use in image processing </li>
												<li id="ft"><a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier Transform</a> - A function that extracts frequency and magnitude information from a given input signal. See also: <a href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform">DFT</a> and <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a></a></li>
												<li id="matlab"><a href="https://www.mathworks.com/products/matlab.html">MATLAB</a> - A programming platform widely used in engineering</li>
												<li id="spectro"><a href="https://en.wikipedia.org/wiki/Spectrogram">Spectrogram</a> - A visual representation of the frequency spectrum over time</li>
												<li id="upsample"><a href="https://en.wikipedia.org/wiki/Upsampling">Upsampling</a> - The process of increasing datapoints in a signal by interpolation. Useful for comparison against signals sampled at higher rates</li>
											</ul>
										</div>
										<div id="source">
											<h3>Source Code</h3>
											<p>
												Take a look at the source code <a href="https://github.com/Cam-H/IoTVoice">here</a>.
											</p>
										</div>

									</section>

							</div>
							<div class="col-3 col-12-medium">

								<!-- Sidebar -->
									<section class="sidenav">
										<header>
											<h2>Index</h2>
										</header>
										<ul class="link-list">
											<li><a href="#introduction">Introduction</a></li>
											<!-- <li><a href="#applications">Introduction</a></li> -->
											<li><a href="#highlights">Technical Highlights</a></li>
											<li><a href="#background">Background</a></li>
											<li><a href="#training">Training</a></li>
											<li><a href="#testing">Testing</a></li>
											<li><a href="#conclusion">Conclusions</a></li>
											<li><a href="#dependencies">Dependencies</a></li>
											<li><a href="#glossary">Glossary</a></li>
											<li><a href="#source">Source Code</a></li>
										</ul>
									</section>
							</div>
						</div>
					</div>
				</section>

			<!-- Footer -->
				<!-- <section id="footer">
					<div class="container">
						<h2>Cameron Hatherell</h2>
							<div>
								<div class="row">
									<div class="left">
										<ul class="link-list last-child">
											<li><a href="#" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
											<li><a href="#" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
										</ul>
									</div>
								</div>
							</div>
					</div>
				</section> -->

			<!-- Copyright -->
				<!-- <div id="copyright">
						&copy; Portfolio - Cameron Hatherell. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a>
				</div> -->

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>